\section{Introduction}

Histopathology is the study and practice of examining biological tissue at a microscopic level with the aim of detecting and/or analysing any disease that may happen to be manifesting therein. It is through the clinical histopathological analysis of tissue resulting from human mammary biopsies that all breast cancers are diagnosed.\par

Early breast lesions are associated with increased risk of invasive recurrence, and present important challenges for diagnosis by histopathology. Notably, in a consultation with clinical pathologists, a majority had cited distinguishing atypical ductal hyperplasias (ADH) from usual epithelial hyperplasias (UEH) and ductal carcinoma \textit{in situ} (DCIS) as the most common challenge among their breast biopsy consultations \citep{putti2005}.\par

Challenges like these can be mitigated in part by computationally assisted detection and diagnosis (CADe/CADx) software, which analyse medical images in a reproducible and quantitative manner with the aim of making the interpretation of these data by clinicians a less complex and subjective practice. While CADe software is sometimes used to aid in the screening of breast mammographies, challenges such as dimensional complexity has historically prevented the use of CADe/x to help interpret histological data \citep{rangayyan2007,madabhushi2009}.  \par

% CNNs are sometimes used for CAD w/ good/bad results

% after next section, information about CAM and CAM-GRAD

\subsection{Convolutional Neural Network Models for the Diagnosis of Breast Cancers}

At the heart of an increasing amount of modern CADe/x solutions are the use of convolutional neural networks (CNNs or ConvNets) \citep{shin2016, cheng2016}. ConvNets are deep machine learning algorithms that use multiple weighted hidden layers of convolutional filters to make decisions about a given input. Recent developments in general-purpose computing on graphics processing units (GPGPU) have made ConvNets, whose use had historically been regarded as ``unrealistic'', practical many years after their first conception \citep{crick1989}. As a result, ConvNets have since been shown to be particularly well suited for the task of classifying and analysing images \citep{ciresan2011, ciresan2012}.\par

ConvNets have been successfully used to create very accurate models for the classification of breast cancer lesions. Binary models for classifying benign and malignant lesions, as well as multi-class models for distinguishing between multiple subtypes of breast lesions from H\&E stained biopsy slides have established with very high ($>90\%$) accuracy \citep{wei2017, han2017}. These models, however, are severely limited in that they classify whole imaging fields as belonging to a single class. These approaches entirely ignore the heterogeneous nature of breast lesions and are entirely ``black-boxes'' for clinicians, offering no added dimensions of information and little understanding as to why the model has interpreted a lesion the way it has. To mitigate this limitation, a classifier would be required to identify, classify and annotate sub-regions that exhibit characteristics of early lesions in medical images of breast biopsies; transparently offering insights into the classifications being made. One such method to so is to generate localisation annotations with the Gradient-weighted Class Activation Mapping algorithm. \par

\subsection{Localisation-Augmented Visualisation of Convolutional Neural Network Using Grad-CAM}

Recent work by \citeauthor{selvaraju2016} has made the interpretation of ConvNets much more clear by visually annotating inputs with general localisations of objects identified by the model. This technique, named Gradient-weighted Class Activation Mapping (Grad-CAM), can produce heatmaps of areas within an input image that, according to a given ConvNet model, are likely to belong to a given class. Grad-CAM is generalisable to most ConvNet architectures, and does not require to be trained on example localisation annotations.\par

Grad-CAM has been since used for the dual purpose of localising classified regions and better understanding differences between classes in practical applications ranging from plant stress phenotyping to classifying colorectal polyps \citep{ghosal2017,korbar2017}.\par

\subsection{Transfer-Learning for Resource Efficient Training of Neural Networks}

Two common limitations of adapting convolutional networks to domain-specific tasks such as classifying medical imagery for computer-aided detection are the large dataset and computational power requirements. These two limitations can be largely addressed by the process of transfer learning, which uses an existing convolutional architecture that has been previously trained (``pre-trained'') on a sufficiently generalised dataset appropriate for the target task \citep{transfer_learning_survey}.The ImageNet ILSVRC2014 dataset is an example of a widely-adopted, readily-available, and comprehensive general-purpose dataset that is commonly the basis of pre-trained models used for transfer-learning image classification tasks \citep{imagenet}.\par

Transfer learning has in-fact been used in a number of computer-aided detection, ranging from thoraco-abdominal lymph node detection and interstitial lung disease classification from chest X-ray and CT scan imaging to classification of skin cancers from dermatoscope imagery \citep{transfer_learning_lungs, transfer_learning_skin}.\par

Transfer learning uses the weights of the many hidden layers of the pre-trained network. The last fully-connected layer of the network is removed from the architecture and a new linear classifier for the network is trained on the new dataset using the pre-trained hidden layers as features.\par

A limitation of transfer learning is that the later, more specialised, hidden layers of the pre-trained network can lead to reduced accuracy of the model if the original dataset the pre-trained layer was trained against is extremely different from the new dataset. This challenge is usually met by an additional process known as fine-tuning, which continues to train the hidden layers of the pre-trained network against the new dataset using backpropagation \citep{yosinski2014}.\par

In this work, we describe the DeepDuct model, a pre-trained ConvNet model (namely, VGG16) fine-tuned on a dataset comprised of histological images of breast biopsies classified across eight different lesion types (the BreakHis dataset), and combines it with the Grad-CAM algorithm to provide general localisation of the various lesions identified, while providing an opportunity to better understand the model.\par

VGG16 is a sixteen weight-layer ConvNet architecture (\textit{Figure \ref{fig:vgg_arch}}) that has been engineered by (and named for) the Visual Geometry Group at the University of Oxford \citep{simonyan2014}. The VGG16 model has been shown to generalise very well to a number of different datasets, and is particularly well suited for localisation tasks, having been awarded first and second place in the classification \& localisation task of the ImageNet ILSVRC2014 contest \citep{imagenet}. This makes the use of the VGG16 architecture well-suited for the function of localisation and classification of medical imagery for the purpose of computational detection and diagnosis.